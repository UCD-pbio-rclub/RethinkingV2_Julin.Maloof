---
title: "Chapter 14"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, autodep = TRUE)
library(rethinking)
library(tidyverse)
```


# Problems

## 14E1

_Add to the following model varying slopes on the predictor $x$_

$$
y_i \sim Normal(\mu_i, \alpha) \\
\mu_i = \alpha_{group[i]} + \beta X_i \\
\alpha_{Group} \sim Normal(\alpha, \sigma_{\alpha}) \\
\alpha \sim Normal(0, 10) \\
\beta \sim Normal(0,1) \\
\sigma \sim HalfCauchy(0, 2) \\
\sigma_\alpha \sim HalfCauchy(0,2) \\
$$
answer:
$$
y_i \sim Normal(\mu_i, \alpha) \\
\mu_i = \alpha_{group[i]} + \beta_{group[i]} X_i \\
\left[
\begin{array}{c}
\alpha_{Group} \\
\beta_{Group}
\end{array}
\right]
\sim MVNormal
\left(
\left[
\begin{array}{c}
\alpha\\
\beta 
\end{array}
\right]
,S
\right) \\
S = \left(
\begin{array}{cc} 
\sigma_\alpha & 0 \\
0 & \sigma_\alpha
\end{array}
\right) 
\mathbf{R}
\left(
\begin{array}{cc} 
\sigma_\alpha & 0 \\
0 & \sigma_\alpha
\end{array}
\right) \\
\alpha \sim Normal(0, 10) \\
\beta \sim Normal(0,1) \\
\sigma \sim HalfCauchy(0, 2) \\
\sigma_\alpha \sim HalfCauchy(0,2) \\
\sigma_\beta \sim HalfCauchy(0,2) \\
\mathbf{R} \sim LKJcorr(2)
$$

## 14E2
_Think up a context in which varying intercepts will be positively correlated with varying slopes. Provide a mechanistic explanation for the correlation._

Competitive plant growth, +/- true shade.  LArge plants will outcomptete smaller plants and get even bigger.  small plants won't be able to complete and will stay small.


## 14E3

_When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes? Explain._

If there is strong correlation between intercept and slope, then effectively fewer parameters need to be estimated (because if you know intercept you know slope).

## 14M1

_Repeat the café robot simulation from the beginning of the chapter.  This time, set rho to zero, so that there is no correlation between intercepts and slopes. How does the posterior distribution of the correlation reflect this change in the underlying simulation?_

### First, from the book:
Simulate some cafes
```{r}
## R code 14.1
a <- 3.5            # average morning wait time
b <- (-1)           # average difference afternoon wait time
sigma_a <- 1        # std dev in intercepts
sigma_b <- 0.5      # std dev in slopes
rho <- (-0.7)       # correlation between intercepts and slopes
```


```{r}
## R code 14.2
Mu <- c( a , b )

## R code 14.5
sigmas <- c(sigma_a,sigma_b) # standard deviations
Rho <- matrix( c(1,rho,rho,1) , nrow=2 ) # correlation matrix

# now matrix multiply to get covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
Sigma
```


```{r}
## R code 14.6
N_cafes <- 20

## R code 14.7
library(MASS)
set.seed(5) # used to replicate example
vary_effects <- mvrnorm( N_cafes , Mu , Sigma )

## R code 14.8
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]
```

now simulate sampling
```{r}
## R code 14.10
set.seed(22)
N_visits <- 10
afternoon <- rep(0:1,N_visits*N_cafes/2)
cafe_id <- rep( 1:N_cafes , each=N_visits )
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- 0.5  # std dev within cafes
wait <- rnorm( N_visits*N_cafes , mu , sigma )
d <- data.frame( cafe=cafe_id , afternoon=afternoon , wait=wait )
```


```{r}
## R code 14.12
m14.1 <- ulam(
    alist(
        wait ~ normal( mu , sigma ),
        mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
        c(a_cafe,b_cafe)[cafe] ~ multi_normal( c(a,b) , Rho , sigma_cafe ),
        a ~ normal(5,2),
        b ~ normal(-1,0.5),
        sigma_cafe ~ exponential(1),
        sigma ~ exponential(1),
        Rho ~ lkj_corr(2)
    ) , data=d , chains=4 , cores=4, log_lik = TRUE )
```

### with rho 0

Simulate some cafes
```{r}
## R code 14.1
a <- 3.5            # average morning wait time
b <- (-1)           # average difference afternoon wait time
sigma_a <- 1        # std dev in intercepts
sigma_b <- 0.5      # std dev in slopes
rho <- (0)       # correlation between intercepts and slopes
```


```{r}
## R code 14.2
Mu <- c( a , b )

## R code 14.5
sigmas <- c(sigma_a,sigma_b) # standard deviations
Rho <- matrix( c(1,rho,rho,1) , nrow=2 ) # correlation matrix

# now matrix multiply to get covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
Sigma
```


```{r}
## R code 14.6
N_cafes <- 20

## R code 14.7
set.seed(5) # used to replicate example
vary_effects <- mvrnorm( N_cafes , Mu , Sigma )

## R code 14.8
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]
```

now simulate sampling
```{r}
## R code 14.10
set.seed(22)
N_visits <- 10
afternoon <- rep(0:1,N_visits*N_cafes/2)
cafe_id <- rep( 1:N_cafes , each=N_visits )
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- 0.5  # std dev within cafes
wait <- rnorm( N_visits*N_cafes , mu , sigma )
d_zero_rho <- data.frame( cafe=cafe_id , afternoon=afternoon , wait=wait )
```


```{r}
## R code 14.12
m14.1alt <- ulam(
    alist(
        wait ~ normal( mu , sigma ),
        mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
        c(a_cafe,b_cafe)[cafe] ~ multi_normal( c(a,b) , Rho , sigma_cafe ),
        a ~ normal(5,2),
        b ~ normal(-1,0.5),
        sigma_cafe ~ exponential(1),
        sigma ~ exponential(1),
        Rho ~ lkj_corr(2)
    ) , data=d_zero_rho , chains=4 , cores=4, log_lik = TRUE )
```

```{r}
precis(m14.1, depth=3, pars=c("a", "b", "Rho"))
precis(m14.1alt, depth = 3, pars=c("a", "b", "Rho"))
```
correlation estimate now ~ zero, as expected.

## 14M2

_Fit this multilevel model (separate pooling) to the simulated café data:_


```{r}
## R code 14.12
m14.M2 <- ulam(
    alist(
        wait ~ normal( mu , sigma ),
        mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
        a_cafe[cafe] ~ normal(a_bar, a_sigma),
        b_cafe[cafe] ~ normal(b_bar, b_sigma),
        a_bar ~ normal(5,2),
        b_bar ~ normal(-1,0.5),
        a_sigma ~ exponential(1),
        b_sigma ~ exponential(1),
        sigma ~ exponential(1)
    ) , data=d , chains=4 , cores=4, log_lik=TRUE )
```

```{r}
compare(m14.1, m14.M2)
```

pretty much the same.  had hoped this would make the case for correlative pooling...

## 14M3

_14M3. Re-estimate the varying slopes model for the UCBadmit data, now using a non-centered pa- rameterization. Compare the efficiency of the forms of the model, using n_eff. Which is better? Which chain sampled faster?_

```{r}
data(UCBadmit)
d <- UCBadmit
dat_list <- list( admit = d$admit,
    applications = d$applications,
    gid = ifelse( d$applicant.gender=="male" , 0L , 1L ),
    dept_id = rep(1:6,each=2) )
```

Varying Slopes model:
```{r}
m14M3a <- ulam(
  alist(
    admit ~ dbinom( applications , p ), 
    logit(p) <- alpha[dept_id] + beta[dept_id]*gid,
    alpha[dept_id] ~ dnorm( 0 , 1.5 ), 
    beta[dept_id] ~ dnorm( 0 , 1.5 )),
  data=dat_list , 
  chains=4 , 
  iter=4000,
  log_lik = TRUE)

```

De-centered model, no pooling
```{r}
m14M3b <- ulam(
  alist(
    admit ~ dbinom( applications , p ), 
    logit(p) <- a_bar + z_a[dept_id]*sigma_a + z_beta[dept_id]*gid*sigma_b,
    a_bar ~ dnorm(0,1.5),
    z_a[dept_id] ~ dnorm( 0 , 1 ), 
    z_beta[dept_id] ~ dnorm( 0 , 1 ),
    sigma_a ~ dexp(1),
    sigma_b ~ dexp(1)),
  data=dat_list , 
  chains=4 , 
  iter=4000,
  log_lik = TRUE)
```

```{r}
precis(m14M3a, depth=2)
```

```{r}
precis(m14M3b, depth=2)
```

```{r}
compare(m14M3a, m14M3b)
```

non-centred is slower to fit, has fewer effective samples and more parameters.


partial pooling, interaction, centered

```{r}
m14M3c <- ulam(
  alist(
    admit ~ dbinom( applications , p ), 
    logit(p) <- alpha[dept_id] + beta[dept_id]*gid,
    c(alpha, beta)[dept_id] ~ multi_normal(c(a_bar, b_bar), Rho, sigma),
    a_bar ~ dnorm( 0 , 1.5 ), 
    b_bar ~ dnorm( 0 , 1.5 ),
    Rho ~ lkj_corr(2),
    sigma ~ dexp(1)),
  data=dat_list , 
  chains=4 , 
  iter=4000,
  log_lik = TRUE)
```

partial pooling, interaction, non-centered

```{r}
m14M3d <- ulam(
  alist(
    admit ~ dbinom( applications , p ), 
    logit(p) <- gamma + coefm[dept_id,1] + coefm[dept_id,2]*gid,
    gamma ~ dnorm(0, 1.5),
    transpars> matrix[dept_id, 2]:coefm <- compose_noncentered(sigma, L_Rho, z),
    matrix[2, dept_id]:z ~ normal(0,1),
    vector[2]:sigma ~ dexp(1),
    cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2),
    gq> matrix[2,2]:Rho <<- multiply_lower_tri_self_transpose(L_Rho)),
  data=dat_list , 
  chains=4 , 
  iter=4000,
  log_lik = TRUE)
```

no divergent transitions

```{r}
precis(m14M3c, depth = 3)
```

```{r}
precis(m14M3d, depth=3)
```

```{r}
compare(m14M3a, m14M3b,m14M3c, m14M3d)
```


## PDF1

_1. Revisit the Bangladesh fertility data, data(bangladesh). Fit a model with both varying intercepts by district_id and varying slopes of urban (as a 0/1 indicator variable) by district_id. You are still predicting use.contraception. Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, in each district, with urban women on one axis and rural on the other, might also help._

```{r}
data("bangladesh")
bangladesh
summary(bangladesh)
```

confirm that district has all its numbers
```{r}
all.equal(unique(bangladesh$district), 1:max(bangladesh$district))
```

it doesn't

```{r}
bangladesh$d_id <- as.numeric(as.factor(bangladesh$district))
all.equal(unique(bangladesh$d_id), 1:max(bangladesh$d_id))

```



```{r}
d <- with(bangladesh, 
          list(use=use.contraception, d_id=d_id, urban=urban))
str(d)
```

start simple
```{r}
mpdf1.1 <- ulam(alist(
  use ~ dbinom(1, p),
  logit(p) <- a[d_id] + b[d_id]*urban,
  a[d_id] ~ dnorm(a_bar, sigma_a),
  b[d_id] ~ dnorm(b_bar, sigma_b),
  a_bar ~ dnorm(0, 1.5),
  b_bar ~ dnorm(0, 1),
  c(sigma_a, sigma_b) ~ dexp(1)),
  data=d,
  cores = 4,
  chains = 4,
  iter = 4000,
  log_lik = TRUE)
```

```{r}
precis(mpdf1.1, depth = 2)
```
Has a hard time estimating sigma_b

now use a mvnorm to look at correlation between intercept and slope.
```{r}
mpdf1.2 <- ulam(alist(
  use ~ dbinom(1, p),
  logit(p) <- a[d_id] + b[d_id]*urban,
  c(a, b)[d_id] ~ multi_normal(c(a_bar, b_bar), Rho, sigma),
  a_bar ~ dnorm(0, 1.5),
  b_bar ~ dnorm(0,1),
  sigma ~ dexp(1),
  Rho ~ lkj_corr(2)),
  data=d,
  cores = 4,
  chains = 4,
  log_lik = TRUE)
```

```{r}
precis(mpdf1.2, depth = 3)
```

```{r}
compare(mpdf1.1, mpdf1.2)
```


de-center it? 
```{r}
mpdf1.3 <- ulam(alist(
  use ~ dbinom(1, p),
  logit(p) <- alphabeta[d_id, 1] + alphabeta[d_id, 2]*urban,
  transpars> matrix[d_id, 2]:alphabeta <- compose_noncentered(sigma, L_Rho, z),
  matrix[2, d_id]:z ~ normal(0, 1),
  vector[2]:sigma ~ dexp(1),
  cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2),
  gq> matrix[2,2]:Rho <<- multiply_lower_tri_self_transpose(L_Rho)),
  data=d,
  cores = 4,
  chains = 4,
  log_lik = TRUE)
```

The decentered model samples more quickly

```{r}
precis(mpdf1.3, depth=3)
```

```{r}
compare(mpdf1.1, mpdf1.2, mpdf1.3)
```

### get model output

```{r}
pred.df <- as_tibble(expand.grid(d_id=unique(d$d_id), urban=c(0,1)))
pred.df
```


```{r}
post <- link(mpdf1.3, pred.df)
str(post)
```

```{r}
pred.df <- pred.df %>%
  mutate(estimate=colMeans(post),
         low89=apply(post,2,HPDI)[1,],
         high89=apply(post,2,HPDI)[2,])
pred.df
```

### plot it

```{r}
pred.df %>%
  mutate(urban=as.character(urban)) %>%
  ggplot(aes(x=urban, y=estimate, group=d_id)) +
  geom_line(alpha=.2) +
  geom_point()
```

Districts with the highest use have negative slopes whereas the rest have positive slopes.  So there is a negative correlation between intercept and slope.  Do we see that in the Rho matrix?

```{r}
precis(mpdf1.3, depth = 3, pars = "Rho")
```

Yes!

```{r}
pred.df %>%
  dplyr::select(d_id, urban, estimate) %>%
  spread(key=urban, value = estimate, sep="_") %>%
  qplot(x=urban_0, y=urban_1, data=.)
```


## PDF2

_2. Now consider the predictor variables age.centered and living.children, also contained in data(bangladesh). Suppose that age influences contraceptive use (changing attitudes) and number of children (older people have had more time to have kids). Number of children may also directly influence contraceptive use. Draw a DAG that reflects these hypothetical relationships. Then build models needed to evaluate the DAG. You will need at least two models. Retain district and urban, as in Problem 1. What do you conclude about the causal influence of age and children?_


```{r}
library(dagitty)
```

```{r}
dagpdf2 <- dagitty(" dag {
  age -> use
  age -> n_child
  n_child -> use
}")

coordinates(dagpdf2) <- list(
  x=c(use=3, age=2, n_child=2),
  y=c(use=2, age=1, n_child=3))

drawdag(dagpdf2)  
```

```{r}
d <- with(bangladesh, 
          list(use=use.contraception, d_id=d_id, urban=urban, age=age.centered, children=living.children))
str(d)
```


Is number of children important?
```{r}
mpdf2.1 <- ulam(alist(
  use ~ dbinom(1, p),
  logit(p) <- alphabeta[d_id, 1] + alphabeta[d_id, 2]*urban + beta_child*children,
  transpars> matrix[d_id, 2]:alphabeta <- compose_noncentered(sigma, L_Rho, z),
  matrix[2, d_id]:z ~ normal(0, 1),
  vector[2]:sigma ~ dexp(1),
  cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2),
  gq> matrix[2,2]:Rho <<- multiply_lower_tri_self_transpose(L_Rho),
  beta_child ~ normal(0,.5)),
  data=d,
  cores = 4,
  chains = 4,
  log_lik = TRUE)
```

```{r}
precis(mpdf2.1)
```

```{r}
precis(mpdf2.1, depth = 3) %>% rownames_to_column("parameter") %>% arrange(desc(Rhat))
```


children still important considering age?
```{r}
mpdf2.2 <- ulam(alist(
  use ~ dbinom(1, p),
  logit(p) <- alphabeta[d_id, 1] + alphabeta[d_id, 2]*urban + beta_child*children + beta_age*age,
  transpars> matrix[d_id, 2]:alphabeta <- compose_noncentered(sigma, L_Rho, z),
  matrix[2, d_id]:z ~ normal(0, 1),
  vector[2]:sigma ~ dexp(1),
  cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2),
  gq> matrix[2,2]:Rho <<- multiply_lower_tri_self_transpose(L_Rho),
  c(beta_age, beta_child) ~ normal(0,.5)),
  data=d,
  cores = 4,
  chains = 4,
  log_lik = TRUE)
```

```{r}
precis(mpdf2.2)
```

Now neither are significant

```{r}
precis(mpdf2.2, depth = 3) %>% rownames_to_column("parameter") %>% arrange(desc(Rhat))
```


_3. Modify any models from Problem 2 that contained that children variable and model the variable now as a monotonic ordered category, like education from the week we did ordered categories. Education in that example had 8 categories. Children here will have fewer (no one in the sample had 8 children). So modify the code appropriately. What do you conclude about the causal influence of each additional child on use of contraception?_

```{r}
min(d$children)
max(d$children)
```

```{r}
d <- with(bangladesh, 
          list(use=use.contraception, d_id=d_id, urban=urban, age=age.centered, children=as.integer(living.children)))
str(d)
alpha <- rep(2,4)
d$alpha <- alpha
```

```{r}
mpdf3.1 <- ulam(alist(
  use ~ ordered_logistic(phi, kappa),
  phi <- alphabeta[d_id, 1] + alphabeta[d_id, 2]*urban + beta_child*sum(delta_j[1:children]),
  transpars> matrix[d_id, 2]:alphabeta <- compose_noncentered(sigma, L_Rho, z),
  matrix[2, d_id]:z ~ normal(0, 1),
  vector[2]:sigma ~ dexp(1),
  cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2),
  gq> matrix[2,2]:Rho <<- multiply_lower_tri_self_transpose(L_Rho),
  beta_child ~ normal(0,.5),
  vector[5]: delta_j <<- append_row(0, delta),
  simplex[4]: delta ~ dirichlet(alpha),
  kappa ~ normal(0, 1.5)),
  data=d,
  cores = 4,
  chains = 4,
  log_lik = TRUE)
```

```{r}
precis(mpdf3.1)
```

```{r}
precis(mpdf3.1) %>% rownames_to_column("parameter") %>% arrange(desc(Rhat))
```


```{r}
mpdf3.2 <- ulam(alist(
  use ~ ordered_logistic(phi, kappa),
  phi <- alphabeta[d_id, 1] + alphabeta[d_id, 2]*urban + beta_child*sum(delta_j[1:children]) + beta_age*age,
  transpars> matrix[d_id, 2]:alphabeta <- compose_noncentered(sigma, L_Rho, z),
  matrix[2, d_id]:z ~ normal(0, 1),
  vector[2]:sigma ~ dexp(1),
  cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2),
  gq> matrix[2,2]:Rho <<- multiply_lower_tri_self_transpose(L_Rho),
  c(beta_child, beta_age) ~ normal(0,.5),
  vector[5]: delta_j <<- append_row(0, delta),
  simplex[4]: delta ~ dirichlet(alpha),
  kappa ~ normal(0, 1.5)),
  data=d,
  cores = 4,
  chains = 4,
  log_lik = TRUE)
```

## Rongkui


_As a curious individual having received above-average number of years of education, I suggest us taking a stab at this dataset with the method used in the textbook to see what we can discover about schooling and earnings. The variables in the dataset are poorly labeled, but the "Descriptive Statistics QOB.txt" file (also available on the economics.mit.edu webpage) provides some useful (although incomplete) clarification. According to the file, the columns we are interested in are:_

* v4: years of education
* v9: log of weekly wage, and 
* v18: quarter of birth

_Feel free to explore the roles of other variables in the dataset too (such as v1 age, v10 marital status, and v19 race). I look forward to seeing everybody's findings!_


```{r}
library(foreign)
qob = read.dta("NEW7080.dta")
```

```{r}
str(qob)
```

```{r}
set.seed(123)
qobs <- sample_frac(qob, 0.05)
qobs2 <- with(qobs, list(edu=standardize(v4), wage=standardize(v9), birth=standardize(v8)))
str(qobs2)
```

education as a predictor of age


```{r}
mr1 <- ulam(
    alist(
        wage ~ dnorm( mu , sigma ),
        mu <- aW + bEW*edu,
        aW ~ dnorm( 0 , 0.2 ),
        bEW ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ) , data=qobs2 , chains=4 , cores=4, log_lik = TRUE )
```


```{r}
precis( mr1 )
```


```{r}
## R code 14.25
mr2 <- ulam(
    alist(
        c(wage, edu) ~ multi_normal( c(muW,muE) , Rho , Sigma ),
        muW <- aW + bEW*edu,
        muE <- aE + bQE*birth,
        c(aW,aE) ~ normal( 0 , 0.2 ),
        c(bEW,bQE) ~ normal( 0 , 0.5 ),
        Rho ~ lkj_corr( 2 ),
        Sigma ~ exponential( 1 )
    ), data=qobs2 , chains=4 , cores=4 )
```


```{r}
precis( mr2 , depth=3 )
```




## Julin

_ Attached are data from an experiment measuring hypocotyl length in ~ 180 natural arabidopsis accessions grown in high and low red:far-red light.  We want to know if there are differences in accessions in their length in high R:FR ("H") and in their response to low R:FR("L").  Also we want to obtain an estimate for hypocotyl length for each accession in high and low R:FR for downstream GWAS analysis._

_Relevant variables:_
* length -- hypocotyl length
* line -- unique ID for each accession (you could also use nativename)
* light -- indicator for high or low RFR
* exp -- two independent experiments were done
* plate -- this is an incomplete block design with a subset (10? 12?) of accessions on each plate.

_Let's try a variety of increasingly complex models:_
* No pooling
* Partial pooling of intercepts and slopes for line and intercepts for plate and experiment, but treat each variable separately (no multivariate component).  you might also consider adding an experiment slope effect
* As 2, but use a multivariate normal model for the line slope and intercept effects
* As 3, but non-centered

_Evaluate and compare the models.  Is there evidence of line, treatment, and line X treatment effects?  How does the magnitude of the experiment and plate effects compare to the line effects?_

```{r}
hyp <- read_csv("hyp.lengths.both.experiments.labels.csv") %>%
  mutate(line_id=as.integer(as.factor(nativename)),
         exp_id=as.integer(as.factor(exp)),
         plate_id=as.integer(as.factor(plate)),
         low_rfr=ifelse(light=="L", 1L, 0L))
hyp.small <- hyp %>% select(length, line_id, low_rfr, plate_id, exp_id, nativename)
hyp.small
```

### no pooling, no interaction

```{r}
hyp.small %>%
  summarize(mean=mean(length), sd=sd(length))
```


```{r}
mj1 <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha[line_id] + beta_rfr * low_rfr,
    alpha[line_id] ~ dnorm(4.1, 5),
    beta_rfr ~ dnorm(0,5),
    sigma ~ dexp(1)),
  data=hyp.small,
  chains=4,
  cores=4,
  log_lik=TRUE)
```

```{r}
#traceplot(mj1, ask=FALSE)
head(precis(mj1, depth=2))
```

```{r}
precis(mj1)
```


### no pooling, yes interaction
```{r}
mj2 <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha[line_id] + beta_rfr[line_id] * low_rfr,
    alpha[line_id] ~ dnorm(4.1, 5),
    beta_rfr[line_id] ~ dnorm(0,5),
    sigma ~ dexp(1)),
  data=hyp.small,
  chains=4,
  cores=4,
  log_lik=TRUE)
```

```{r}
#traceplot(mj2, ask=FALSE)
head(precis(mj2, depth = 2))
```

```{r}
compare(mj1, mj2)
```


### 2: Partial pooling of intercepts and slopes for line and intercepts for plate and experiment, but treat each variable separately

first, leave plate and experiment out of it
```{r}
mj3a <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha[line_id] + beta_rfr[line_id] * low_rfr,
    alpha[line_id] ~ dnorm(a_bar, sigma_a),
    beta_rfr[line_id] ~ dnorm(b_bar, sigma_b),
    a_bar ~ dnorm(4, 5),
    b_bar ~ dnorm(0, 3),
    c(sigma, sigma_a, sigma_b) ~ dexp(1)),
  data=hyp.small,
  chains=4,
  cores=4,
  log_lik=TRUE)
```

```{r}
precis(mj3a)
```

```{r}
compare(mj2, mj3a)
```


add experiment and plate #fail!
```{r, eval=FALSE}
mj3b <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha_line[line_id] + beta_rfr[line_id] * low_rfr + alpha_plate[plate_id] + alpha_exp[exp_id],
    alpha_line[line_id] ~ dnorm(al_bar, sigma_al),
    beta_rfr[line_id] ~ dnorm(b_bar, sigma_b),
    alpha_plate[plate_id] ~ dnorm(0, sigma_plate),
    alpha_exp[exp_id] ~ dnorm(0, sigma_exp),
    al_bar ~ dnorm(4, 5),
    b_bar ~ dnorm(0, 3),
    c(sigma, sigma_al, sigma_b, sigma_plate, sigma_exp) ~ dexp(1)),
  data=hyp.small,
  chains=4,
  cores=4,
  log_lik=TRUE)
```

```{r, eval=FALSE}
precis(mj3b, depth=2) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(20)
```

plate only?

```{r, eval=FALSE}
mj3b2 <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha_line[line_id] + beta_rfr[line_id] * low_rfr + alpha_plate[plate_id],
    alpha_line[line_id] ~ dnorm(al_bar, sigma_al),
    beta_rfr[line_id] ~ dnorm(b_bar, sigma_b),
    alpha_plate[plate_id] ~ dnorm(0, sigma_plate),
    al_bar ~ dnorm(4, 5),
    b_bar ~ dnorm(0, 3),
    c(sigma, sigma_al, sigma_b, sigma_plate) ~ dexp(1)),
  data=hyp.small,
  chains=4,
  cores=4,
  iter = 4000,
  log_lik=TRUE)
```

```{r}
precis(mj3b2)
```

```{r}
precis(mj3b2, depth=2) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(20)
```


#### exp only:

```{r}
mj3b3 <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha_line[line_id] + beta_rfr[line_id] * low_rfr + alpha_exp[exp_id],
    alpha_line[line_id] ~ dnorm(al_bar, sigma_al),
    beta_rfr[line_id] ~ dnorm(b_bar, sigma_b),
    alpha_exp[exp_id] ~ dnorm(0, sigma_exp),
    al_bar ~ dnorm(4, 5),
    b_bar ~ dnorm(0, 3),
    c(sigma, sigma_al, sigma_b, sigma_exp) ~ dexp(1)),
  data=hyp.small,
  chains=4,
  cores=4,
  log_lik=TRUE)
```

```{r}
precis(mj3b3)
```

```{r}
precis(mj3b3, depth=2) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(20)
```


can we diagnose the problem?

```{r}
pnames <- precis(mj3b3, depth=2) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(10) %>% pull(parameter)

pairs(mj3b3, pars=pnames)
```

Hah!

plate is nested within experiment, so what if tried to model it that way?


```{r, eval=FALSE}
mj3b4 <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha_line[line_id] + beta_rfr[line_id] * low_rfr + alpha_plate[plate_id],
    alpha_line[line_id] ~ dnorm(al_bar, sigma_al),
    beta_rfr[line_id] ~ dnorm(b_bar, sigma_b),
    alpha_plate[plate_id] ~ dnorm(a_plate_bar[exp_id], sigma_plate),
    al_bar ~ dnorm(4, 5),
    b_bar ~ dnorm(0, 3),
    a_plate_bar[exp_id] ~ dnorm(0,1),
    c(sigma, sigma_al, sigma_b, sigma_plate) ~ dexp(1)),
  data=hyp.small,
  chains=4,
  cores=4,
  iter = 4000,
  log_lik=TRUE)
```
doesn't work "Chain 1: Exception: normal_lpdf: Random variable has dimension = 152, expecting dimension = 18031; a function was called with arguments of different scalar, array, vector, or matrix types, and they were not consistently sized;  all arguments must be scalars or multidimensional values of the same shape. "

need to work on this more.

```{r}
compare(mj2, mj3a, mj3b2)
```
Wow!


```{r}
rbind(precis(mj2), head(precis(mj2, depth=2), 20))
```

```{r}
rbind(precis(mj3a), head(precis(mj3a, depth=2), 20))
```

```{r}
rbind(precis(mj3b2), head(precis(mj3b2, depth=2), 20))
```


would be nice to compare line estimates from the three models...later...


### 3: As 2, but use a multivariate normal model for the line slope and intercept effects

leave plate and exp out for now

```{r}
mj4 <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha_line[line_id] + beta_rfr[line_id] * low_rfr,
    c(alpha_line, beta_rfr)[line_id] ~ multi_normal(c(a_bar, b_bar), Rho, sigma2),
    a_bar ~ dnorm(4, 5),
    b_bar ~ dnorm(0, 3),
    c(sigma, sigma2) ~ dexp(1),
    Rho ~ lkj_corr(2)),
  data=hyp.small,
  chains=4,
  cores=4,
  log_lik=TRUE)
```

```{r}
precis(mj4)
```

```{r}
precis(mj4, depth=3) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(20)
```


add plate

```{r}
mj4a <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha_line[line_id] + beta_rfr[line_id] * low_rfr + alpha_plate[plate_id],
    c(alpha_line, beta_rfr)[line_id] ~ multi_normal(c(a_bar, b_bar), Rho, sigma2),
    a_bar ~ dnorm(4, 5),
    b_bar ~ dnorm(0, 3),
    alpha_plate[plate_id] ~ dnorm(0, sigma_plate),
    c(sigma, sigma2, sigma_plate) ~ dexp(1),
    Rho ~ lkj_corr(2)),
  data=hyp.small,
  chains=4,
  cores=4,
  iter=4000,
  log_lik=TRUE)
```

```{r}
precis(mj4a) 
```

```{r}
precis(mj4a, depth = 3) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(20)
```

```{r}
compare(mj2, mj3a, mj3b2, mj4, mj4a)
```

Is there a correlation between slope and intercept?
```{r}
precis(mj4, depth=3, pars = "Rho")
```

```{r}
precis(mj4a, depth=3, pars = "Rho")
```

Yes, negative correlation

### 4: As 3, but non-centered

```{r}
mj5 <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha + beta*low_rfr + 
      alpha_beta_line[line_id, 1] + 
      alpha_beta_line[line_id, 2] * low_rfr,
    transpars> matrix[line_id, 2]:alpha_beta_line <- 
      compose_noncentered(sigma_line, L_Rho_line, z_line),
    matrix[2, line_id]:z_line ~ normal(0,1),
    alpha ~ dnorm(4,5),
    beta ~ dnorm(0,5),
    vector[2]:sigma_line ~ dexp(1),
    sigma ~ dexp(1),
    cholesky_factor_corr[2]:L_Rho_line ~ lkj_corr_cholesky(2),
    gq> matrix[2,2]:Rho_line <<- multiply_lower_tri_self_transpose(L_Rho_line)),
  data=hyp.small,
  chains=4,
  cores=4,
  iter=4000,
  log_lik=TRUE)
```

slower than centered(!)

```{r}
precis(mj5) 
```

```{r}
precis(mj5) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(20)
```


```{r}
mj5a <- ulam(
  alist(
    length ~ dnorm(u, sigma),
    u <- alpha + beta*low_rfr + 
      alpha_beta_line[line_id, 1] + 
      alpha_beta_line[line_id, 2] * low_rfr +
      alpha_plate[plate_id] ~ dnorm(0, sigma_plate),
    transpars> matrix[line_id, 2]:alpha_beta_line <- 
      compose_noncentered(sigma_line, L_Rho_line, z_line),
    matrix[2, line_id]:z_line ~ normal(0,1),
    alpha ~ dnorm(4,5),
    beta ~ dnorm(0,5),
    vector[2]:sigma_line ~ dexp(1),
    c(sigma,sigma_plate) ~ dexp(1),
    cholesky_factor_corr[2]:L_Rho_line ~ lkj_corr_cholesky(2),
    gq> matrix[2,2]:Rho_line <<- multiply_lower_tri_self_transpose(L_Rho_line)),
  data=hyp.small,
  chains=4,
  cores=4,
  iter=4000,
  log_lik=TRUE)
```

```{r}
precis(mj5a) 
```

```{r}
precis(mj5a) %>% rownames_to_column(var="parameter") %>% arrange(desc(Rhat)) %>% head(20)
```


```{r}
compare(mj4,mj5, mj4a, mj5a)
```


## 14M4
_Use WAIC to compare the Gaussian process model of Oceanic tools to the models fit to the same data in Chapter 11. Pay special attention to the effective numbers of parameters, as estimated by WAIC._


### Chapter 14: Guassian process model


```{r}
## R code 14.36
# load the distance matrix
library(rethinking)
data(islandsDistMatrix)

# display (measured in thousands of km)
Dmat <- islandsDistMatrix
colnames(Dmat) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
round(Dmat,1)
```


```{r}
## R code 14.38
data(Kline2) # load the ordinary data, now with coordinates
d <- Kline2
d$society <- 1:10 # index observations

dat_list <- list(
    T = d$total_tools,
    P = d$population,
    society = d$society,
    Dmat=islandsDistMatrix )

m14.7 <- ulam(
    alist(
        T ~ dpois(lambda),
        lambda <- (a*P^b/g)*exp(k[society]),
        vector[10]:k ~ multi_normal( 0 , SIGMA ),
        matrix[10,10]:SIGMA <- cov_GPL2( Dmat , etasq , rhosq , 0.01 ),
        c(a,b,g) ~ dexp( 1 ),
        etasq ~ dexp( 2 ),
        rhosq ~ dexp( 0.5 )
    ), data=dat_list , chains=4 , cores=4 , iter=2000, log_lik = TRUE )
```

### Chapter 11
```{r}
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 )
dat2 <- list( T=d$total_tools, P=d$population, cid=d$contact_id )
m11.11 <- ulam(
    alist(
        T ~ dpois( lambda ),
        lambda <- exp(a[cid])*P^b[cid]/g,
        a[cid] ~ dnorm(1,1),
        b[cid] ~ dexp(1),
        g ~ dexp(1)
    ), data=dat2 , chains=4 , log_lik=TRUE )
```

The original data in chapter 11 used scaled log(P) but chapter 14 does not...

```{r}
precis(m11.11, depth = 2)
precis(m14.7)
```

```{r}
compare(m11.11, m14.7)
```


# Book

Simulate some cafes
```{r}
## R code 14.1
a <- 3.5            # average morning wait time
b <- (-1)           # average difference afternoon wait time
sigma_a <- 1        # std dev in intercepts
sigma_b <- 0.5      # std dev in slopes
rho <- (-0.7)       # correlation between intercepts and slopes
```


```{r}
## R code 14.2
Mu <- c( a , b )

## R code 14.3
cov_ab <- sigma_a*sigma_b*rho
Sigma <- matrix( c(sigma_a^2,cov_ab,cov_ab,sigma_b^2) , ncol=2 )

## R code 14.4
matrix( c(1,2,3,4) , nrow=2 , ncol=2 )

## R code 14.5
sigmas <- c(sigma_a,sigma_b) # standard deviations
Rho <- matrix( c(1,rho,rho,1) , nrow=2 ) # correlation matrix

# now matrix multiply to get covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
Sigma
```


```{r}
## R code 14.6
N_cafes <- 20

## R code 14.7
library(MASS)
set.seed(5) # used to replicate example
vary_effects <- mvrnorm( N_cafes , Mu , Sigma )

## R code 14.8
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]

## R code 14.9
plot( a_cafe , b_cafe , col=rangi2 ,
    xlab="intercepts (a_cafe)" , ylab="slopes (b_cafe)" )

# overlay population distribution
library(ellipse)
for ( l in c(0.1,0.3,0.5,0.8,0.99) )
    lines(ellipse(Sigma,centre=Mu,level=l),col=col.alpha("black",0.2))
```

now simulate sampling
```{r}
## R code 14.10
set.seed(22)
N_visits <- 10
afternoon <- rep(0:1,N_visits*N_cafes/2)
cafe_id <- rep( 1:N_cafes , each=N_visits )
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- 0.5  # std dev within cafes
wait <- rnorm( N_visits*N_cafes , mu , sigma )
d <- data.frame( cafe=cafe_id , afternoon=afternoon , wait=wait )
d
```


```{r}
## R code 14.11
R <- rlkjcorr( 1e4 , K=2 , eta=2 )
dens( R[,1,2] , xlab="correlation" )
```


```{r}
## R code 14.12
m14.1 <- ulam(
    alist(
        wait ~ normal( mu , sigma ),
        mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
        c(a_cafe,b_cafe)[cafe] ~ multi_normal( c(a,b) , Rho , sigma_cafe ),
        a ~ normal(5,2),
        b ~ normal(-1,0.5),
        sigma_cafe ~ exponential(1),
        sigma ~ exponential(1),
        Rho ~ lkj_corr(2)
    ) , data=d , chains=4 , cores=4 )
```


```{r}
## R code 14.13
post <- extract.samples(m14.1)
dens( post$Rho[,1,2] )
```


```{r}
## R code 14.14
# compute unpooled estimates directly from data
a1 <- sapply( 1:N_cafes ,
        function(i) mean(wait[cafe_id==i & afternoon==0]) )
b1 <- sapply( 1:N_cafes ,
        function(i) mean(wait[cafe_id==i & afternoon==1]) ) - a1

# extract posterior means of partially pooled estimates
post <- extract.samples(m14.1)
a2 <- apply( post$a_cafe , 2 , mean )
b2 <- apply( post$b_cafe , 2 , mean )

# plot both and connect with lines
plot( a1 , b1 , xlab="intercept" , ylab="slope" ,
    pch=16 , col=rangi2 , ylim=c( min(b1)-0.1 , max(b1)+0.1 ) ,
    xlim=c( min(a1)-0.1 , max(a1)+0.1 ) )
points( a2 , b2 , pch=1 )
for ( i in 1:N_cafes ) lines( c(a1[i],a2[i]) , c(b1[i],b2[i]) )

## R code 14.15
# compute posterior mean bivariate Gaussian
Mu_est <- c( mean(post$a) , mean(post$b) )
rho_est <- mean( post$Rho[,1,2] )
sa_est <- mean( post$sigma_cafe[,1] )
sb_est <- mean( post$sigma_cafe[,2] )
cov_ab <- sa_est*sb_est*rho_est
Sigma_est <- matrix( c(sa_est^2,cov_ab,cov_ab,sb_est^2) , ncol=2 )

# draw contours
library(ellipse)
for ( l in c(0.1,0.3,0.5,0.8,0.99) )
    lines(ellipse(Sigma_est,centre=Mu_est,level=l),
        col=col.alpha("black",0.2))
```


```{r}
## R code 14.16
# convert varying effects to waiting times
wait_morning_1 <- (a1)
wait_afternoon_1 <- (a1 + b1)
wait_morning_2 <- (a2)
wait_afternoon_2 <- (a2 + b2)

# plot both and connect with lines
plot( wait_morning_1 , wait_afternoon_1 , xlab="morning wait" ,
    ylab="afternoon wait" , pch=16 , col=rangi2 ,
    ylim=c( min(wait_afternoon_1)-0.1 , max(wait_afternoon_1)+0.1 ) ,
    xlim=c( min(wait_morning_1)-0.1 , max(wait_morning_1)+0.1 ) )
points( wait_morning_2 , wait_afternoon_2 , pch=1 )
for ( i in 1:N_cafes )
    lines( c(wait_morning_1[i],wait_morning_2[i]) ,
    c(wait_afternoon_1[i],wait_afternoon_2[i]) )
abline( a=0 , b=1 , lty=2 )

## R code 14.17
# now shrinkage distribution by simulation
v <- mvrnorm( 1e4 , Mu_est , Sigma_est )
v[,2] <- v[,1] + v[,2] # calculate afternoon wait
Sigma_est2 <- cov(v)
Mu_est2 <- Mu_est
Mu_est2[2] <- Mu_est[1]+Mu_est[2]

# draw contours
library(ellipse)
for ( l in c(0.1,0.3,0.5,0.8,0.99) )
    lines(ellipse(Sigma_est2,centre=Mu_est2,level=l),
        col=col.alpha("black",0.5))
```


```{r, eval=FALSE}
## R code 14.18
library(rethinking)
data(chimpanzees)
d <- chimpanzees
d$block_id <- d$block
d$treatment <- 1L + d$prosoc_left + 2L*d$condition

dat <- list(
    L = d$pulled_left,
    tid = d$treatment,
    actor = d$actor,
    block_id = as.integer(d$block_id) )
```


```{r, eval=FALSE}
m14.2 <- ulam(
    alist(
        L ~ binomial(1,p),
        logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],

        # adaptive priors
        vector[4]:alpha[actor] ~ multi_normal(0,Rho_actor,sigma_actor), #prior for the effect of each actor in each treatment
        vector[4]:beta[block_id] ~ multi_normal(0,Rho_block,sigma_block),

        # fixed priors
        g[tid] ~ dnorm(0,1),
        sigma_actor ~ dexp(1),
        Rho_actor ~ dlkjcorr(4),
        sigma_block ~ dexp(1),
        Rho_block ~ dlkjcorr(4)
    ) , data=dat , chains=4 , cores=4 )
```


```{r, eval=FALSE}
## R code 14.19
m14.3 <- ulam(
    alist(
        L ~ binomial(1,p),
        logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],

        # adaptive priors - non-centered
        transpars> matrix[actor,4]:alpha <-
                compose_noncentered( sigma_actor , L_Rho_actor , z_actor ),
        transpars> matrix[block_id,4]:beta <-
                compose_noncentered( sigma_block , L_Rho_block , z_block ),
        matrix[4,actor]:z_actor ~ normal( 0 , 1 ),
        matrix[4,block_id]:z_block ~ normal( 0 , 1 ),

        # fixed priors
        g[tid] ~ normal(0,1),
        vector[4]:sigma_actor ~ dexp(1),
        cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky( 2 ),
        vector[4]:sigma_block ~ dexp(1),
        cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky( 2 ),

        # compute ordinary correlation matrixes from Cholesky factors
        gq> matrix[4,4]:Rho_actor <<- multiply_lower_tri_self_transpose(L_Rho_actor),
        gq> matrix[4,4]:Rho_block <<- multiply_lower_tri_self_transpose(L_Rho_block)
    ) , data=dat , chains=4 , cores=4 , log_lik=TRUE )
```

```{r}
precis(m14.2, depth=3)
```

```{r}
precis(m14.3, depth = 3)
```


```{r, eval=FALSE}
## R code 14.20
# extract n_eff values for each model
neff_nc <- precis(m14.3,3,pars=c("alpha","beta"))$n_eff
neff_c <- precis(m14.2,3,pars=c("alpha","beta"))$n_eff
plot( neff_c , neff_nc , xlab="centered (default)" ,
    ylab="non-centered (cholesky)" , lwd=1.5 )
abline(a=0,b=1,lty=2)
```


```{r, eval=FALSE}
## R code 14.21
precis( m14.3 , depth=2 , pars=c("sigma_actor","sigma_block") )
```


```{r, eval=FALSE}
## R code 14.22
# compute mean for each actor in each treatment
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )

# generate posterior predictions using link
datp <- list(
    actor=rep(1:7,each=4) ,
    tid=rep(1:4,times=7) ,
    block_id=rep(5,times=4*7) )
p_post <- link( m14.3 , data=datp )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )

# set up plot
plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
    ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )

xo <- 0.1 # offset distance to stagger raw data and predictions
# raw data
for ( j in (1:7)[-2] ) {
    lines( (j-1)*4+c(1,3)-xo , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
    lines( (j-1)*4+c(2,4)-xo , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28-xo , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28-xo , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )

yoff <- 0.175
text( 1-xo , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2-xo , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3-xo , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4-xo , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )

# posterior predictions
for ( j in (1:7)[-2] ) {
    lines( (j-1)*4+c(1,3)+xo , p_mu[(j-1)*4+c(1,3)] , lwd=2 )
    lines( (j-1)*4+c(2,4)+xo , p_mu[(j-1)*4+c(2,4)] , lwd=2 )
}
for ( i in 1:28 ) lines( c(i,i)+xo , p_ci[,i] , lwd=1 )
points( 1:28+xo , p_mu , pch=16 , col="white" , cex=1.3 )
points( 1:28+xo , p_mu , pch=c(1,1,16,16) )
```


```{r, eval=FALSE}
## R code 14.23
set.seed(73)
N <- 500
U_sim <- rnorm( N )
Q_sim <- sample( 1:4 , size=N , replace=TRUE )
E_sim <- rnorm( N , U_sim + Q_sim )
W_sim <- rnorm( N , U_sim + 0*E_sim )
dat_sim <- list(
    W=standardize(W_sim) ,
    E=standardize(E_sim) ,
    Q=standardize(Q_sim) )
```


```{r, eval=FALSE}
## R code 14.24
m14.4 <- ulam(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- aW + bEW*E,
        aW ~ dnorm( 0 , 0.2 ),
        bEW ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ) , data=dat_sim , chains=4 , cores=4 )
precis( m14.4 )
```


```{r, eval=FALSE}
## R code 14.25
m14.5 <- ulam(
    alist(
        c(W,E) ~ multi_normal( c(muW,muE) , Rho , Sigma ),
        muW <- aW + bEW*E,
        muE <- aE + bQE*Q,
        c(aW,aE) ~ normal( 0 , 0.2 ),
        c(bEW,bQE) ~ normal( 0 , 0.5 ),
        Rho ~ lkj_corr( 2 ),
        Sigma ~ exponential( 1 )
    ), data=dat_sim , chains=4 , cores=4 )
precis( m14.5 , depth=3 )
```


```{r, eval=FALSE}
## R code 14.26
m14.4x <- ulam( m14.4 , data=dat_sim , chains=4 , cores=4 )
m14.5x <- ulam( m14.5 , data=dat_sim , chains=4 , cores=4 )

## R code 14.27
set.seed(73)
N <- 500
U_sim <- rnorm( N )
Q_sim <- sample( 1:4 , size=N , replace=TRUE )
E_sim <- rnorm( N , U_sim + Q_sim )
W_sim <- rnorm( N , -U_sim + 0.2*E_sim )
dat_sim <- list(
    W=standardize(W_sim) ,
    E=standardize(E_sim) ,
    Q=standardize(Q_sim) )
```


```{r, eval=FALSE}
## R code 14.28
library(dagitty)
dagIV <- dagitty( "dag{
    E -> W
    E <- U -> W
    Q -> E
}")
instrumentalVariables( dagIV , exposure="E" , outcome="W" )
```


```{r, eval=FALSE}
## R code 14.29
library(rethinking)
data(KosterLeckie)
library(tidyverse)
```

```{r, eval=FALSE}
kl_dyads %>% arrange(did)
```


```{r, eval=FALSE}
## R code 14.30
kl_data <- list(
    N = nrow(kl_dyads),
    N_households = max(kl_dyads$hidB),
    did = kl_dyads$did,
    hidA = kl_dyads$hidA,
    hidB = kl_dyads$hidB,
    giftsAB = kl_dyads$giftsAB,
    giftsBA = kl_dyads$giftsBA
)
```


```{r, eval=FALSE}
m14.4 <- ulam(
    alist(
        giftsAB ~ poisson( lambdaAB ),
        giftsBA ~ poisson( lambdaBA ),
        log(lambdaAB) <- a + gr[hidA,1] + gr[hidB,2] + d[did,1] ,
        log(lambdaBA) <- a + gr[hidB,1] + gr[hidA,2] + d[did,2] ,
        a ~ normal(0,1),

       ## gr matrix of varying effects
        vector[2]:gr[N_households] ~ multi_normal(0,Rho_gr,sigma_gr),
        Rho_gr ~ lkj_corr(4),
        sigma_gr ~ exponential(1),

       ## dyad effects
        transpars> matrix[N,2]:d <-
                compose_noncentered( rep_vector(sigma_d,2) , L_Rho_d , z ),
        matrix[2,N]:z ~ normal( 0 , 1 ),
        cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky( 8 ),
        sigma_d ~ exponential(1),

       ## compute correlation matrix for dyads
        gq> matrix[2,2]:Rho_d <<- Chol_to_Corr( L_Rho_d )
    ), data=kl_data , chains=4 , cores=4 , iter=2000 )
```


```{r, eval=FALSE}
## R code 14.31
precis( m14.4 , depth=3 , pars=c("Rho_gr","sigma_gr") )
```


```{r, eval=FALSE}
## R code 14.32
post <- extract.samples( m14.4 )
g <- sapply( 1:25 , function(i) post$a + post$gr[,i,1] )
r <- sapply( 1:25 , function(i) post$a + post$gr[,i,2] )
Eg_mu <- apply( exp(g) , 2 , mean )
Er_mu <- apply( exp(r) , 2 , mean )
```


```{r, eval=FALSE}
## R code 14.33
plot( NULL , xlim=c(0,8.6) , ylim=c(0,8.6) , xlab="generalized giving" ,
    ylab="generalized receiving" , lwd=1.5 )
abline(a=0,b=1,lty=2)

# ellipses
library(ellipse)
for ( i in 1:25 ) {
    Sigma <- cov( cbind( g[,i] , r[,i] ) )
    Mu <- c( mean(g[,i]) , mean(r[,i]) )
    for ( l in c(0.5) ) {
        el <- ellipse( Sigma , centre=Mu , level=l )
        lines( exp(el) , col=col.alpha("black",0.5) )
    }
}
# household means
points( Eg_mu , Er_mu , pch=21 , bg="white" , lwd=1.5 )
```


```{r, eval=FALSE}
## R code 14.34
precis( m14.4 , depth=3 , pars=c("Rho_d","sigma_d") )
```


```{r, eval=FALSE}
## R code 14.35
dy1 <- apply( post$d[,,1] , 2 , mean )
dy2 <- apply( post$d[,,2] , 2 , mean )
plot( dy1 , dy2 )
```


```{r, eval=FALSE}
## R code 14.36
# load the distance matrix
library(rethinking)
data(islandsDistMatrix)

# display (measured in thousands of km)
Dmat <- islandsDistMatrix
colnames(Dmat) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
round(Dmat,1)
```


```{r, eval=FALSE}
## R code 14.37
# linear
curve( exp(-1*x) , from=0 , to=4 , lty=2 ,
    xlab="distance" , ylab="correlation" )

# squared
curve( exp(-1*x^2) , add=TRUE )
```


```{r, eval=FALSE}
## R code 14.38
data(Kline2) # load the ordinary data, now with coordinates
d <- Kline2
d$society <- 1:10 # index observations

dat_list <- list(
    T = d$total_tools,
    P = d$population,
    society = d$society,
    Dmat=islandsDistMatrix )

m14.7 <- ulam(
    alist(
        T ~ dpois(lambda),
        lambda <- (a*P^b/g)*exp(k[society]),
        vector[10]:k ~ multi_normal( 0 , SIGMA ),
        matrix[10,10]:SIGMA <- cov_GPL2( Dmat , etasq , rhosq , 0.01 ),
        c(a,b,g) ~ dexp( 1 ),
        etasq ~ dexp( 2 ),
        rhosq ~ dexp( 0.5 )
    ), data=dat_list , chains=4 , cores=4 , iter=2000 )
```


```{r, eval=FALSE}
## R code 14.39
precis( m14.7 , depth=3 )
```


```{r, eval=FALSE}
## R code 14.40
post <- extract.samples(m14.7)

# plot the posterior median covariance function
plot( NULL , xlab="distance (thousand km)" , ylab="covariance" ,
    xlim=c(0,10) , ylim=c(0,2) )

# compute posterior mean covariance
x_seq <- seq( from=0 , to=10 , length.out=100 )
pmcov <- sapply( x_seq , function(x) post$etasq*exp(-post$rhosq*x^2) )
pmcov_mu <- apply( pmcov , 2 , mean )
lines( x_seq , pmcov_mu , lwd=2 )

# plot 60 functions sampled from posterior
for ( i in 1:50 )
    curve( post$etasq[i]*exp(-post$rhosq[i]*x^2) , add=TRUE ,
        col=col.alpha("black",0.3) )
```


```{r, eval=FALSE}
## R code 14.41
# compute posterior median covariance among societies
K <- matrix(0,nrow=10,ncol=10)
for ( i in 1:10 )
    for ( j in 1:10 )
        K[i,j] <- median(post$etasq) *
                  exp( -median(post$rhosq) * islandsDistMatrix[i,j]^2 )
diag(K) <- median(post$etasq) + 0.01

## R code 14.42
# convert to correlation matrix
Rho <- round( cov2cor(K) , 2 )
# add row/col names for convenience
colnames(Rho) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
rownames(Rho) <- colnames(Rho)
Rho
```


```{r, eval=FALSE}
## R code 14.43
# scale point size to logpop
psize <- d$logpop / max(d$logpop)
psize <- exp(psize*1.5)-2

# plot raw data and labels
plot( d$lon2 , d$lat , xlab="longitude" , ylab="latitude" ,
    col=rangi2 , cex=psize , pch=16 , xlim=c(-50,30) )
labels <- as.character(d$culture)
text( d$lon2 , d$lat , labels=labels , cex=0.7 , pos=c(2,4,3,3,4,1,3,2,4,2) )

# overlay lines shaded by Rho
for( i in 1:10 )
    for ( j in 1:10 )
        if ( i < j )
            lines( c( d$lon2[i],d$lon2[j] ) , c( d$lat[i],d$lat[j] ) ,
                lwd=2 , col=col.alpha("black",Rho[i,j]^2) )
```


```{r, eval=FALSE}
## R code 14.44
# compute posterior median relationship, ignoring distance
logpop.seq <- seq( from=6 , to=14 , length.out=30 )
#lambda <- sapply( logpop.seq , function(lp) exp( post$a + post$b*lp) )
#lambda <- sapply( logpop.seq , function(lp) exp( post$a + post$b*lp  - post$g) )
lambda <- sapply( logpop.seq , function(lp) post$a*exp(lp)^post$b/post$g )

# why not (post$a*lp^post$b/post$g) ??
lambda.median <- apply( lambda , 2 , median )
lambda.PI80 <- apply( lambda , 2 , PI , prob=0.8 )

# plot raw data and labels
plot( d$logpop , d$total_tools , col=rangi2 , cex=psize , pch=16 ,
    xlab="log population" , ylab="total tools" ,)
text( d$logpop , d$total_tools , labels=labels , cex=0.7 ,
    pos=c(4,3,4,2,2,1,4,4,4,2) )

# display posterior predictions
lines( logpop.seq , lambda.median , lty=2 )
lines( logpop.seq , lambda.PI80[1,] , lty=2 )
lines( logpop.seq , lambda.PI80[2,] , lty=2 )

# overlay correlations
for( i in 1:10 )
    for ( j in 1:10 )
        if ( i < j )
            lines( c( d$logpop[i],d$logpop[j] ) ,
                   c( d$total_tools[i],d$total_tools[j] ) ,
                   lwd=2 , col=col.alpha("black",Rho[i,j]^2) )
```


```{r, eval=FALSE}
## R code 14.45
m14.7nc <- ulam(
    alist(
        T ~ dpois(lambda),
        lambda <- (a*P^b/g)*exp(k[society]),

        # non-centered Gaussian Process prior
        transpars> vector[10]: k <<- L_SIGMA * z,
        vector[10]: z ~ normal( 0 , 1 ),
        transpars> matrix[10,10]: L_SIGMA <<- cholesky_decompose( SIGMA ),
        transpars> matrix[10,10]: SIGMA <- cov_GPL2( Dmat , etasq , rhosq , 0.01 ),

        c(a,b,g) ~ dexp( 1 ),
        etasq ~ dexp( 2 ),
        rhosq ~ dexp( 0.5 )
    ), data=dat_list , chains=4 , cores=4 , iter=2000 )

## R code 14.46
library(rethinking)
data(Primates301)
data(Primates301_nex)

# plot it using ape package - install.packages('ape') if needed
library(ape)
plot( ladderize(Primates301_nex) , type="fan" , font=1 , no.margin=TRUE ,
    label.offset=1 , cex=0.5 )

## R code 14.47
d <- Primates301
d$name <- as.character(d$name)
dstan <- d[ complete.cases( d$group_size , d$body , d$brain ) , ]
spp_obs <- dstan$name

## R code 14.48
dat_list <- list(
    N_spp = nrow(dstan),
    M = standardize(log(dstan$body)),
    B = standardize(log(dstan$brain)),
    G = standardize(log(dstan$group_size)),
    Imat = diag( nrow(dstan) )
)

m14.8 <- ulam(
    alist(
        B ~ multi_normal( mu , SIGMA ),
        mu <- a + bM*M + bG*G,
        matrix[N_spp,N_spp]: SIGMA <- Imat * sigma_sq,
        a ~ normal( 0 , 1 ),
        c(bM,bG) ~ normal( 0 , 0.5 ),
        sigma_sq ~ exponential( 1 )
    ), data=dat_list , chains=4 , cores=4 )
precis( m14.8 )

## R code 14.49
library(ape)
tree_trimmed <- keep.tip( Primates301_nex, spp_obs )
Rbm <- corBrownian( phy=tree_trimmed )
V <- vcv(Rbm)
Dmat <- cophenetic( tree_trimmed )
plot( Dmat , V , xlab="phylogenetic distance" , ylab="covariance" )

## R code 14.50
# put species in right order
dat_list$V <- V[ spp_obs , spp_obs ]
# convert to correlation matrix
dat_list$R <- dat_list$V / max(V)

# Brownian motion model
m14.9 <- ulam(
    alist(
        B ~ multi_normal( mu , SIGMA ),
        mu <- a + bM*M + bG*G,
        matrix[N_spp,N_spp]: SIGMA <- R * sigma_sq,
        a ~ normal( 0 , 1 ),
        c(bM,bG) ~ normal( 0 , 0.5 ),
        sigma_sq ~ exponential( 1 )
    ), data=dat_list , chains=4 , cores=4 )
precis( m14.9 )

## R code 14.51
# add scaled and reordered distance matrix
dat_list$Dmat <- Dmat[ spp_obs , spp_obs ] / max(Dmat)

m14.10 <- ulam(
    alist(
        B ~ multi_normal( mu , SIGMA ),
        mu <- a + bM*M + bG*G,
        matrix[N_spp,N_spp]: SIGMA <- cov_GPL1( Dmat , etasq , rhosq , 0.01 ),
        a ~ normal(0,1),
        c(bM,bG) ~ normal(0,0.5),
        etasq ~ half_normal(1,0.25),
        rhosq ~ half_normal(3,0.25)
    ), data=dat_list , chains=4 , cores=4 )
precis( m14.10 )

## R code 14.52
post <- extract.samples(m14.10)
plot( NULL , xlim=c(0,max(dat_list$Dmat)) , ylim=c(0,1.5) ,
    xlab="phylogenetic distance" , ylab="covariance" )

# posterior
for ( i in 1:30 )
    curve( post$etasq[i]*exp(-post$rhosq[i]*x) , add=TRUE , col=rangi2 )

# prior mean and 89% interval
eta <- abs(rnorm(1e3,1,0.25))
rho <- abs(rnorm(1e3,3,0.25))
d_seq <- seq(from=0,to=1,length.out=50)
K <- sapply( d_seq , function(x) eta*exp(-rho*x) )
lines( d_seq , colMeans(K) , lwd=2 )
shade( apply(K,2,PI) , d_seq )
text( 0.5 , 0.5 , "prior" )
text( 0.2 , 0.1 , "posterior" , col=rangi2 )

## R code 14.53
S <- matrix( c( sa^2 , sa*sb*rho , sa*sb*rho , sb^2 ) , nrow=2 )


```

