---
title: "R Notebook"
output: html_notebook
---

```{r}
library(rethinking)
library(tidyverse)
```

# Problems

Practice:
## 15E1
_Rewrite the Oceanic tools model (from Chapter 11) below so that it assumes measured error on the log population sizes of each society._

Assuming P is standardized:
$$
T_i \sim\ Poisson(mu_i) \\
log(mu_i) = \alpha + \beta*log(P_{true,i}) \\
P_{obs,i} \sim normal(P_{true,i}, PSE_i)\\
P_{true,i} \sim normal(0, 1) \\
\alpha \sim normal(0,10) \\
\beta \sim normal(0,1)
$$

## 15E2
_Rewrite the same model so that it allows imputation of missing values for log population. There aren’t any missing values in the variable, but you can still write down a model formula that would imply imputation, if any values were missing._

$$
T_i \sim\ Poisson(mu_i) \\
log(mu_i) = \alpha + \beta*log(P_{impute,i}) \\
P_{impute,i} \sim normal(\nu, \sigma_P)\\
\alpha \sim normal(0,10) \\
\beta \sim normal(0,1) \\
\nu \sim normal(0,1) \\
\sigma_P \sim exp(1)
$$

## 15M3. 
_Repeatthedivorcedatameasurementerrormodels,butthistimedoublethestandarderrors. Can you explain how doubling the standard errors impacts inference?_

### original
```{r}
data(WaffleDivorce)
d <- WaffleDivorce
## R code 15.3
dlist <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = d$Divorce.SE / sd( d$Divorce ),
    M = standardize( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.1 <- ulam(
    alist(
        D_obs ~ dnorm( D_true , D_sd ),
        vector[N]:D_true ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M,
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=dlist , chains=4 , cores=4 )
```

### Double standard error
```{r}
## R code 15.3
dlist2 <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = 2*d$Divorce.SE / sd( d$Divorce ),
    M = standardize( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.1.2 <- ulam(
    alist(
        D_obs ~ dnorm( D_true , D_sd ),
        vector[N]:D_true ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M,
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=dlist2 , chains=4 , cores=4 )
```

```{r}
precis(m15.1)
precis(m15.1.2)
```

Doubling the measurement standard error noticebly reduces sigma, because it allows the observations to get "shrunk" more.

## 15H1

_The data in data(elephants) are counts of matings observed for bull elephants of differing ages. There is a strong positive relationship between age and matings. However, age is not always assessed accurately. First, fit a Poisson model predicting MATINGS with AGE as a predictor. Second, assume that the observed AGE values are uncertain and have a standard error of ±5 years. Re-estimate the relationship between MATINGS and AGE, incorporating this measurement error. Compare the inferences of the two models._

```{r}
data("elephants")
str(elephants)
```

```{r}
plot(elephants$AGE, elephants$MATINGS)
```

```{r}
d <- list(M = elephants$MATINGS, A = standardize(elephants$AGE))
```

check the priors
```{r}
age <- seq(min(d$A), max(d$A), by = .1)
priorpred1 <- tibble(
  id = 1:100,
  a = rnorm(100, 0, 1),
  b = rnorm(100, 0, 1))

priorpred1 <- priorpred1 %>%
  mutate(pred=map2(a,b, ~ exp(.x + .y*age)),
         pred=map(pred, ~ tibble(M=.,A=age))) %>% # add age column
  unnest(pred)
priorpred1
```

```{r}
priorpred1 %>%
  ggplot(aes(x=A, y=M)) +
  geom_line(aes(group=id), alpha=.1)+
  geom_point(data=as.data.frame(d)) +
  scale_y_log10()
```


```{r}
mh1.1 <- ulam(
  alist(M ~ dpois(lambda),
        log(lambda) <- a + bA*A,
        a ~ dnorm(0,1),
        bA ~ dnorm(0,1)),
  data = d,
  chains = 4,
  cores = 4)
```

```{r}
precis(mh1.1)
```

```{r}
pred1 <- link(mh1.1, data=list(A=age))
dim(pred1)
pred.plot1 <- tibble(
  A=age,
  M=apply(pred1,2,mean),
  lower=apply(pred1,2,HPDI)[1,],
  upper=apply(pred1,2,HPDI)[2,],
)
```


```{r}
pred.plot1 %>% 
  ggplot(aes(x=A,y=M)) +
  geom_ribbon(aes(ymin=lower,ymax=upper),fill="gray80") +
    geom_line() +
  geom_point(data=as.data.frame(d))
```

### now with standard error

```{r}
sem_std <- 5/sd(elephants$AGE)
d$N <- length(d$A)
d$sem_std <- sem_std
mh1.2 <- ulam(
  alist(M ~ dpois(lambda),
        log(lambda) <- a + bA*Aest[i],
        A ~ dnorm(Aest, sem_std),
        vector[N]:Aest ~ dnorm(0,1),
        a ~ dnorm(0,1),
        bA ~ dnorm(0,1)),
  data = d,
  chains = 4,
  cores = 4)
```


```{r}
precis(mh1.1)
precis(mh1.2)
```

bA somewhat larger; and larger error

```{r}
pred2 <- link(mh1.2, data=list(A=age))
dim(pred2)
pred.plot2 <- tibble(
  A=age,
  M=apply(pred2,2,mean),
  lower=apply(pred2,2,HPDI)[1,],
  upper=apply(pred2,2,HPDI)[2,],
)

pred.plot2 %>% 
  ggplot(aes(x=A,y=M)) +
  geom_ribbon(aes(ymin=lower,ymax=upper),fill="gray80") +
    geom_line() +
  geom_point(data=as.data.frame(d))
```

Not working well; because of variation due to SE?  Maybe need to pull out coefficents and make predictions from that?

```{r}
post <- extract.samples(mh1.2)
```


## 15H2

_Repeatthemodelfittingproblemabove,nowincreasingtheassumedstandarderroronAGE. How large does the standard error have to get before the posterior mean for the coefficient on AGE reaches zero?_

```{r}
dofit <- function(SE, dtmp=d )
{
  sem_std <- SE/sd(elephants$AGE)
dtmp$N <- length(d$A)
dtmp$sem_std <- sem_std
m.tmp <- ulam(
  alist(M ~ dpois(lambda),
        log(lambda) <- a + bA*Aest[i],
        A ~ dnorm(Aest, sem_std),
        vector[N]:Aest ~ dnorm(0,1),
        a ~ dnorm(0,1),
        bA ~ dnorm(0,1)),
  data = dtmp,
  chains = 4,
  cores = 4)
p.tmp <- precis(m.tmp) %>% data.frame() %>% rownames_to_column("coef")
return(p.tmp)
}

H2.table <- tibble(SE=c(5, 10, 15, 20, 25, 50, 100))
```


```{r}
H2.table <- H2.table %>%
  mutate(results=map(SE, dofit))
```

```{r}
H2.table %>%  unnest(results) %>% arrange(desc(coef), SE)
```
Things start to fall about by SE of about 20.

# Book

```{r, eval=FALSE}
## R code 15.1
# simulate a pancake and return randomly ordered sides
sim_pancake <- function() {
    pancake <- sample(1:3,1)
    sides <- matrix(c(1,1,1,0,0,0),2,3)[,pancake]
    sample(sides)
}

# sim 10,000 pancakes
pancakes <- replicate( 1e4 , sim_pancake() )
up <- pancakes[1,]
down <- pancakes[2,]

# compute proportion 1/1 (BB) out of all 1/1 and 1/0
num_11_10 <- sum( up==1 )
num_11 <- sum( up==1 & down==1 )
num_11/num_11_10
```


```{r, eval=FALSE}
## R code 15.2
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce

# points
plot( d$Divorce ~ d$MedianAgeMarriage , ylim=c(4,15) ,
    xlab="Median age marriage" , ylab="Divorce rate" )

# standard errors
for ( i in 1:nrow(d) ) {
    ci <- d$Divorce[i] + c(-1,1)*d$Divorce.SE[i]
    x <- d$MedianAgeMarriage[i]
    lines( c(x,x) , ci )
}
```


```{r, eval=FALSE}
## R code 15.3
dlist <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = d$Divorce.SE / sd( d$Divorce ),
    M = standardize( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.1 <- ulam(
    alist(
        D_obs ~ dnorm( D_true , D_sd ),
        vector[N]:D_true ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M,
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=dlist , chains=4 , cores=4 )
```


```{r, eval=FALSE}
## R code 15.4
precis( m15.1 , depth=2 )
```


```{r, eval=FALSE}
## R code 15.5
dlist <- list(
    D_obs = standardize( d$Divorce ),
    D_sd = d$Divorce.SE / sd( d$Divorce ),
    M_obs = standardize( d$Marriage ),
    M_sd = d$Marriage.SE / sd( d$Marriage ),
    A = standardize( d$MedianAgeMarriage ),
    N = nrow(d)
)

m15.2 <- ulam(
    alist(
        D_obs ~ dnorm( D_est , D_sd ),
        vector[N]:D_est ~ dnorm( mu , sigma ),
        mu <- a + bA*A + bM*M_est[i],
        M_obs ~ dnorm( M_est , M_sd ),
        vector[N]:M_est ~ dnorm( 0 , 1 ),
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp( 1 )
    ) , data=dlist , chains=4 , cores=4 )
```


```{r, eval=FALSE}
## R code 15.6
post <- extract.samples( m15.2 )
D_est <- apply( post$D_est , 2 , mean )
M_est <- apply( post$M_est , 2 , mean )
plot( dlist$M_obs , dlist$D_obs , pch=16 , col=rangi2 ,
    xlab="marriage rate (std)" , ylab="divorce rate (std)" )
points( M_est , D_est )
for ( i in 1:nrow(d) )
    lines( c( dlist$M_obs[i] , M_est[i] ) , c( dlist$D_obs[i] , D_est[i] ) )
```


```{r, eval=FALSE}
## R code 15.7
N <- 500
A <- rnorm(N)
M <- rnorm(N,-A)
D <- rnorm(N,A)
A_obs <- rnorm(N,A)
```


```{r, eval=FALSE}
## R code 15.8
N <- 100
S <- rnorm( N )
H <- rbinom( N , size=10 , inv_logit(S) )

## R code 15.9
D <- rbern( N ) # dogs completely random
Hm <- H
Hm[D==1] <- NA

## R code 15.10
D <- ifelse( S > 0 , 1 , 0 )
Hm <- H
Hm[D==1] <- NA

## R code 15.11
set.seed(501)
N <- 1000
X <- rnorm(N)
S <- rnorm(N)
H <- rbinom( N , size=10 , inv_logit( 2 + S - 2*X ) )
D <- ifelse( X > 1 , 1 , 0 )
Hm <- H
Hm[D==1] <- NA

## R code 15.12
dat_list <- list(
    H = H,
    S = S )

m15.3 <- ulam(
    alist(
        H ~ binomial( 10 , p ),
        logit(p) <- a + bS*S,
        a ~ normal( 0 , 1 ),
        bS ~ normal( 0 , 0.5 )
    ), data=dat_list , chains=4 )
precis( m15.3 )

## R code 15.13
dat_list0 <- list(
    H = H[D==0],
    S = S[D==0] )

m15.4 <- ulam(
    alist(
        H ~ binomial( 10 , p ),
        logit(p) <- a + bS*S,
        a ~ normal( 0 , 1 ),
        bS ~ normal( 0 , 0.5 )
    ), data=dat_list0 , chains=4 )
precis( m15.4 )

## R code 15.14
D <- ifelse( abs(X) < 1 , 1 , 0 )

## R code 15.15
N <- 100
S <- rnorm(N)
H <- rbinom( N , size=10 , inv_logit(S) )
D <- ifelse( H < 5 , 1 , 0 )
Hm <- H
Hm[D==1] <- NA

## R code 15.16
library(rethinking)
data(milk)
d <- milk
d$neocortex.prop <- d$neocortex.perc / 100
d$logmass <- log(d$mass)

## R code 15.17
dat_list <- list(
    K = standardize( d$kcal.per.g ),
    B = standardize( d$neocortex.prop ),
    M = standardize( d$logmass )
)

m15.3 <- ulam(
    alist(
        K ~ dnorm( mu , sigma ),
        mu <- a + bB*B + bM*M,
        B ~ dnorm( nu , sigma_B ),
        c(a,nu) ~ dnorm( 0 , 0.5 ),
        c(bB,bM) ~ dnorm( 0, 0.5 ),
        sigma_B ~ dexp( 1 ),
        sigma ~ dexp( 1 )
    ) , data=dat_list , chains=4 , cores=4 )

## R code 15.18
precis( m15.3 , depth=2 )

## R code 15.19
obs_idx <- which( !is.na(d$neocortex.prop) )
dat_list_obs <- list(
    K = dat_list$K[obs_idx],
    B = dat_list$B[obs_idx],
    M = dat_list$M[obs_idx]
)
m15.4 <- ulam(
    alist(
        K ~ dnorm( mu , sigma ),
        mu <- a + bB*B + bM*M,
        B ~ dnorm( nu , sigma_B ),
        c(a,nu) ~ dnorm( 0 , 0.5 ),
        c(bB,bM) ~ dnorm( 0, 0.5 ),
        sigma_B ~ dexp( 1 ),
        sigma ~ dexp( 1 )
    ) , data=dat_list_obs , chains=4 , cores=4 )
precis( m15.4 )

## R code 15.20
plot( coeftab(m15.3,m15.4) , pars=c("bB","bM") )

## R code 15.21
post <- extract.samples( m15.3 )
B_impute_mu <- apply( post$B_impute , 2 , mean )
B_impute_ci <- apply( post$B_impute , 2 , PI )

# B vs K
plot( dat_list$B , dat_list$K , pch=16 , col=rangi2 ,
    xlab="neocortex percent (std)" , ylab="kcal milk (std)" )
miss_idx <- which( is.na(dat_list$B) )
Ki <- dat_list$K[miss_idx]
points( B_impute_mu , Ki )
for ( i in 1:12 ) lines( B_impute_ci[,i] , rep(Ki[i],2) )

# M vs B
plot( dat_list$M , dat_list$B , pch=16 , col=rangi2 ,
    ylab="neocortex percent (std)" , xlab="log body mass (std)" )
Mi <- dat_list$M[miss_idx]
points( Mi , B_impute_mu )
for ( i in 1:12 ) lines( rep(Mi[i],2) , B_impute_ci[,i] )

## R code 15.22
m15.5 <- ulam(
    alist(
       # K as function of B and M
        K ~ dnorm( mu , sigma ),
        mu <- a + bB*B_merge + bM*M,

       # M and B correlation
        MB ~ multi_normal( c(muM,muB) , Rho_BM , Sigma_BM ),
        matrix[29,2]:MB <<- append_col( M , B_merge ),

       # define B_merge as mix of observed and imputed values
        vector[29]:B_merge <- merge_missing( B , B_impute ),

       # priors
        c(a,muB,muM) ~ dnorm( 0 , 0.5 ),
        c(bB,bM) ~ dnorm( 0, 0.5 ),
        sigma ~ dexp( 1 ),
        Rho_BM ~ lkj_corr(2),
        Sigma_BM ~ exponential(1)
    ) , data=dat_list , chains=4 , cores=4 )
precis( m15.5 , depth=3 , pars=c("bM","bB","Rho_BM" ) )

## R code 15.23
B_missidx <- which( is.na( dat_list$B ) )

## R code 15.24
data(Moralizing_gods)
str(Moralizing_gods)

## R code 15.25
table( Moralizing_gods$moralizing_gods , useNA="always" )

## R code 15.26
symbol <- ifelse( Moralizing_gods$moralizing_gods==1 , 16 , 1 )
symbol <- ifelse( is.na(Moralizing_gods$moralizing_gods) , 4 , symbol )
color <- ifelse( is.na(Moralizing_gods$moralizing_gods) , "black" , rangi2 )
plot( Moralizing_gods$year , Moralizing_gods$population , pch=symbol ,
    col=color , xlab="Time (year)" , ylab="Population size" , lwd=1.5 )

## R code 15.27
dmg <- Moralizing_gods
table( gods=dmg$moralizing_gods , literacy=dmg$writing , useNA="always" )

## R code 15.28
dmg <- Moralizing_gods
haw <- which( dmg$polity=="Big Island Hawaii" )
t( dmg[ haw , c("year","population","writing","moralizing_gods") ] )

## R code 15.29
set.seed(9)
N_houses <- 100L
alpha <- 5
beta <- (-3)
k <- 0.5
r <- 0.2
cat <- rbern( N_houses , k )
notes <- rpois( N_houses , alpha + beta*cat )
R_C <- rbern( N_houses , r )
cat_obs <- cat
cat_obs[R_C==1] <- (-9L)

## R code 15.30
dat <- list(
    notes = notes,
    cat = cat_obs,
    RC = R_C,
    N = as.integer(N_houses) )

m15.6 <- ulam(
    alist(
        # singing bird model
        ## cat known present/absent:
        notes|RC==0 ~ poisson( lambda ),
        log(lambda) <- a + b*cat,
        ## cat NA:
        notes|RC==1 ~ custom( log_sum_exp(
                log(k) + poisson_lpmf( notes | exp(a + b) ),
                log(1-k) + poisson_lpmf( notes | exp(a) )
            ) ),

        # priors
        a ~ normal(0,1),
        b ~ normal(0,0.5),

        # sneaking cat model
        cat|RC==0 ~ bernoulli(k),
        k ~ beta(2,2)
    ), data=dat , chains=4 , cores=4 )

## R code 15.31
m15.7 <- ulam(
    alist(
        # singing bird model
        notes|RC==0 ~ poisson( lambda ),
        notes|RC==1 ~ custom( log_sum_exp(
                log(k) + poisson_lpmf( notes | exp(a + b) ),
                log(1-k) + poisson_lpmf( notes | exp(a) )
            ) ),
        log(lambda) <- a + b*cat,
        a ~ normal(0,1),
        b ~ normal(0,0.5),

        # sneaking cat model
        cat|RC==0 ~ bernoulli(k),
        k ~ beta(2,2),

        # imputed values
        gq> vector[N]:PrC1 <- exp(lpC1)/(exp(lpC1)+exp(lpC0)),
        gq> vector[N]:lpC1 <- log(k) + poisson_lpmf( notes[i] | exp(a+b) ),
        gq> vector[N]:lpC0 <- log(1-k) + poisson_lpmf( notes[i] | exp(a) )
    ), data=dat , chains=4 , cores=4 )

## R code 15.32
set.seed(100)
x <- c( rnorm(10) , NA )
y <- c( rnorm(10,x) , 100 )
d <- list(x=x,y=y)
```
